---
title: "Looming learning task power analysis"
format: html
---

```{r setup}
#| include: false
#| 
library(tidyverse)
library(rlang)

# Handles an arbitrary number of response options
# because it requires the responses to be fed in as vector names

get_qs <- function (ws) {
  qs <- ws %>% 
    # the q for each action type should be in a column
    summarize(across(-feature, mean))
  
  return (qs)
}

choose_response <- function (qs, beta = 5) {
  
  action_values <- qs %>% 
    pivot_longer(cols = everything(), names_to = "action", values_to = "q") %>% 
    # softmax the qs
    # beta is inverse temperature
    mutate(p = exp(beta * q) / sum(exp(beta * q)))
  
  choice <- sample(action_values$action, size = 1, prob = action_values$p)
  
  return (choice)
}

# In the new multi-weight system,
# Each full-state/action pair gets a single Q, like usual q-learning
# But it's derived from Ws (sub-Qs, sort of) for each of the component features
# And those Ws are what actually need to get updated for the next trial
# And remember, for example there is a separate W for rabbit-dodgeleft and rabbit-dodgeright (same feature level, different actions) AND rabbit-dodgeleft and snake-dodgeleft (different feature levels, same action)
# We need to feed the state-specific Q for the observed outcome in to update the Ws
# But only the updated Ws come out (because the next state-specific Q will come from the Ws)
update_ws <- function (action, reward, q_prev, ws_prev, alpha = 0.5) {

  # the qs for semantic and origin will be shared across states with the same level
  # so that, e.g., both rabbit-left-dodgeleft and rabbit-right-dodgeleft trials will update the rabbit-dodgeleft value
  ws_updated <- ws_prev %>% 
    # use rlang to choose the column name with a variable
    # corresponding to the chosen action
    mutate(!!action := !!sym(action) + alpha * (reward - q_prev))
  
  return (ws_updated)
}


generate_trials <- function (n_trials, params, outcomes) {
  
  w_names <- c(unique(params$semantic),
               unique(params$origin),
               unique(params$conjunction))
  
  w_initial <- expand_grid(feature = w_names,
                           outcome = outcomes) %>%
    mutate(w = 0) %>% 
    pivot_wider(names_from = outcome, values_from = w)
  
  trials <- tibble(trial_num = 1:n_trials) %>% 
    mutate(semantic = sample(unique(params$semantic), size = n(), replace = TRUE),
           origin = sample(unique(params$origin), size = n(), replace = TRUE)) %>% 
    left_join(hyperparameters %>% select(-conjunction), by = c("semantic", "origin")) %>% 
    mutate(destination = map_chr(prob_left, \(x) {
             out <- sample(outcomes,
                           size = 1,
                           prob = c(x, 1 - x))
             return (out)
           }),
           choice = NA_character_,
           # If necessary, unnest_wider() can be used to pull each state's qs out into their own column
           ws = w_initial %>% 
             list() %>% 
             rep(times = n()))
  
  return (trials)
}

simulate_choices <- function (trials, punish_value, alpha, beta) {
  for (i in 1:nrow(trials)) {
    if (i == 1) {
      # this should be ok because on the first pass, the q-values should all be 0
      ws_prev <- trials$ws[[i]]
    } else {
      ws_prev <- trials$ws[[i-1]]
    }
    
    features_relevant <- c(trials$semantic[i],
                           trials$origin[i],
                           paste(trials$semantic[i], trials$origin[i], sep = "_"))
    # get only the ws for the posterior relevant features
    # given that a semantic and an origin have already been shown
    ws_relevant <- ws_prev %>% 
      filter(feature %in% features_relevant)
    
    # also separate the irrelevant features
    # to bind back onto the updated relevant features later
    ws_irrelevant <- ws_prev %>% 
      filter(!(feature %in% features_relevant))
    
    qs_prev <- get_qs(ws_relevant)
    
    trials$choice[i] <- choose_response(qs = qs_prev, beta = beta)
    
    # If you chose WRONG and got smacked
    # punishment value for getting hit is -1
    if (trials$choice[i] == trials$destination[i]) {
      outcome_value <- punish_value
    } else {
      outcome_value <- 0
    }
    
    q_chosen <- qs_prev[[trials$choice[i]]]
    
    ws_updated <- update_ws(action = trials$choice[i],
                            reward = outcome_value, 
                            q_prev = q_chosen,
                            ws_prev = ws_relevant,
                            alpha = alpha)
    
    trials$ws[[i]] <- bind_rows(ws_updated, ws_irrelevant)
    
  }
  
  return (trials)
}
```

## Setting hyperparameters

```{r}
levels_semantic <- c("rabbit", "snake")
levels_origin <- c("left", "right")
# for right now just see what happens if there's two choices
levels_destination <- c("left", "right")

hyperparameters <- crossing(semantic = levels_semantic,
                            origin = levels_origin) %>% 
  mutate(conjunction = paste(semantic, origin, sep = "_"),
         # prob = case_when(semantic == "rabbit" & destination == "left" ~ 0.6, 
        #                   semantic == "snake" & destination == "right" ~ 0.6,
          #                TRUE ~ 0.2),
         prob_left = if_else(semantic == "rabbit", 0.8, 0.2))

```

```{r}
trials <- generate_trials(n_trials = 200, 
                          params = hyperparameters, 
                          outcomes = levels_destination) %>% 
  simulate_choices(punish_value = -1, alpha = 0.6, beta = 5)
```

# At what point do the q-values approximately stabilize?

```{r}
trials %>% 
  select(trial_num, ws) %>% 
  unnest(ws) %>% 
  pivot_longer(cols = !!levels_destination,
               names_to = "choice",
               values_to = "w") %>% 
  ggplot(aes(x = trial_num, y = w, color = choice)) + 
  geom_hline(yintercept = 0, linetype = "dotted") +
  geom_line() + 
  facet_wrap(~ feature, ncol = 1)
```

